---
modules:
  - "[[mocs/Modules/Dynamic Programming|Module 3: Dynamic Programming]]"
tags:
  - status/draft
---

# Overview


When to use DP:

→ That’s an **optimization** problem 
→ This gives a sequential structure — the sequence 1..n.
→ This is a choice structure.

---


First stage is to find an appropriate optimal substructure property and corresponding recurrence relation.

Dynamic Programming Pattern: 
1. Decompose the problem into subproblems. 
2. Recursively define the value of a solution of a subproblem by the value of solutions of smaller subproblems. 
3. Compute the value of the solutions for subproblems from smaller to larger. 


 We need to find some way to (recursively) break the problem into smaller subproblems, and we need to determine a recursive formulation, which represents the optimum solution to each problem in terms of solutions to the subproblems. 

 1. Characterize the structure of an optimal solution (Optimal Substructure). 
2. Recursively define the value of a solution of a subproblem by the value of solutions of smaller subproblems from top-down . (Recursive Solution)


When developing a dynamic-programming algorithm, we follow a sequence of four steps: 
3. Characterize the structure of an optimal solution (Optimal Substructure). 
	2. The first question is: what do the subproblems look like and how many are there?
	3. The first step is to find the optimal substructure and then use it to construct an optimal solution to the problem from optimal solutions to subproblems
	4. Decompose the problem into subproblems. We want to break the problem into subproblems that exhibit some kind of *optimal substructure property*. In other words, we want to find subproblems whose solutions can be combined to solve the global problem. 
4. Recursively define the value of an optimal solution from top-down (Recursive Solution)
	1. The second step is to derive a recursive relationship between the subproblems by reasoning in a top-down manner: how do we solve a given subproblem assuming you’ve already solved all the smaller subproblems? how to solve a generic subproblem given the solution to “earlier” subproblems? how can we solve for an instance $(i, j)$ of the problem in terms of the smaller problems $(i-1, j-1), (i-1, j), (i, j-1)$?
5. Compute the value of an optimal solution from bottom-up (The algorithm).
	- This requires finding an ordering of the table elements so that when a table item is calculated using the recurrence relation, all the table values needed by the recurrence relation have already been calculated.
6. Construct an optimal solution from computed information.
7. (Optional) Use the choices made (arrows) to reconstruct an actual solution.


Usually, the subproblem solutions are arranged in an array or table, where a subproblem depends on ones earlier in the array/table. Here are several variants to be aware of:

- The subproblems are in a one-dimensional array; or a multidimensional array (matrix).
- A subproblem is the same as the original problem, just on a smaller input; or the subproblem is slightly different. Similarly, computing the final solution is just returning the final subproblem solution; or it requires more computation depending on the subproblem.
- The subproblems correspond directly to prefixes of the input; or we create extra index variables to break down the problem in an artificial way. Examples of the latter: single-use knapsack, Floyd-Warshall.

---

**Dynamic Programming**
- Similar to divide-and-conquer. 
	- solves problem by combining solution to sub-problems 
- Different from divide-and-conquer. 
	- sub-problems are not independent 
	- save solutions to repeated sub-problems in table

**Recipe**
- Characterise structure of problem. Identify the optimal substructure property 
- Recursively define value of optimal solution.
- Compute value of optimal solution.
- Construct optimal solution from computed information.

As a general structure, your solution should include the following descriptions:

- A definition of your subproblem.  
- A well-defined recurrence with respect to your subproblem definition and base cases.
- The base case(s) and the final solution(s).  
- The order of computation.  
- An analysis of its correctness and running time.


Your solution should include:
- A clear subproblem definition. 
- Base cases for your subproblem definition.
- A well-defined recurrence with respect to your subproblem definition and base cases.
- Some output that solves the original problem, as a function of the results generated by your recurrence.
- A correct order of computation, with respect to your recurrence.
- Time complexity analysis for your algorithm. 
- Justification that your algorithm solves the problem correctly, with specific reference to the correctness of the base case(s), recurrence and overall answer.

Recall that there are two different techniques we can use to implement a dynamic programming solution; memoization and tabulation.

- **Memoization** is where we add caching to a function (that has no side effects). In dynamic programming, it is typically used on **recursive** functions for a **top-down** solution that starts with the initial problem and then recursively calls itself to solve smaller problems.
- **Tabulation** uses a table to keep track of subproblem results and works in a **bottom-up** manner: solving the smallest subproblems before the large ones, in an **iterative** manner. Often, people use the words "tabulation" and "dynamic programming" interchangeably.

To design a tractable (non-exponential) algorithm for an optimization problem, we can either:

1. Identifying a greedy algorithm
2. Use dynamic programming

There is no guarantee that either is possible. Additionally, greedy algorithms are strictly less common than dynamic programming algorithms and are often more difficult to identify. However, *if* a greedy algorithm exists, then it will almost always be better than a dynamic programming one. You should, therefore, at least give some thought to the potential existence of a greedy algorithm before jumping straight into dynamic programming.

## Dynamic Programming

In dynamic programming, we solve a problem by breaking it into smaller subproblems and then combine the solutions together to solve the original problem. However, dynamic programming improves the running time in the following way: *each subproblem has previously been solved in the past*. In this way, we do not need to recompute solutions; instead, we can simply store them in memory so that they can be accessed quickly. However, this benefit means that not every problem can be solved with dynamic programming. 

### Properties

> [!caution]
> These properties are not properties of the problem statement, but properties of the solution itself! That is: 
> - Does an optimal solution to this problem have an optimal substructure?

In particular, there exist a dynamic programming solution if the optimal solution to the problem exhibits the following two properties:

- <b>Property 1.</b> **Optimal substructure.**

This means that optimal solutions arise from optimal sub-solutions. In particular, this implies the correctness of any recurrence that correctly solves the subproblems.

Some definitions from the web:
- A problem has optimal substructure if it can be broken into smaller problems such that the optimal solution to the big problem can be deduced from the optimal solution to the smaller problems.  
- Optimal substructure means *an optimal solution to a problem of size n* is based on optimal solutions to smaller problems of size n' < n.

> "If I already have the best (optimal) answers for some a smaller part of the problem, how can I use/combine them to get the best answer for the full problem?"

- <b>Property 2.</b> **Overlapping subproblems.**  

This means that computing the solution to the current subproblem require subproblems that we have previously solved.

*This means that dynamic programming really does make a difference in the time complexity!*

### Template

- In a nutshell, dynamic programming is recursion without repetition. Dynamic programming algorithms store the solutions of intermediate subproblems, often but not always in some kind of array or table. M
- Dynamic programming is not about filling in tables. It’s about smart recursion!
- As long as we memoize the correct recurrence,


Once we have derived a recurrence, we can transform it into a dynamic programming algorithm following 
- Subproblems
- Memoization structure:
- Dependencies:
- Evaluation order:
- Space and time:

Dynamic programming formalizes this idea:

> “A big problem can be built up from smaller problems if the optimal solution to the big one depends only on the optimal solutions to smaller ones.”

So your job is to **find the right smaller problems** (the *subproblem definition*) and the **right way to combine them** (the *recurrence*).

The reasoning always follows this pipeline:

1. Define subproblems precisely (what each one means).
2. Ask: if I knew all smaller subproblems, could I deduce the larger one?
3. Break the larger problem into a few **cases** that together cover all possibilities.
4. From those cases, express the value of the large subproblem in terms of smaller ones.
5. Fill out all subproblems in a consistent order.


The cases are not arbitrary—they arise from the *structure* of the problem: how choices in the last step affect the solution.

Dynamic programming algorithms are best developed in two distinct stages: 
- Fderiving a recurrence relation 



1. <b>Formulate the problem recursively from the <u>top down</u>.</b> Write down a recursive formula or algorithm for the whole problem in terms of the answers to smaller subproblems. This is the hard part! Basically, we want to reason from the top-down, to describe how the larger problems depend on the smaller ones.
	1. **Characterise Optimality.** The first step is to find a way to recursively break the problem down into smaller subproblems. We want to find a set of subproblems that exhibit some kind of *optimal substructure property*; such that we can create an optimal solution to the problem from the solutions of those subproblems. Once we have defined our subproblems, we want to derive a recurrence relation between the subproblems from the top-down:
		1. The idea is to compute the optimal solution for some instance/subproblem, assuming that we already know the solutions of the smaller subproblems. 
		2. Then we reason backwards to consider its dependencies. 
		3. Assume there exists an optimal solution for a particular subproblem. Focus on its last choice/element/part and consider all possible states for this last part (included/excluded, matched/unmatched, etc.). For each state, 
		4. Then we reason backwards: what are all the different cases for this last part? If I remove this part, what does the remaining problem look like? What smaller solutions can I use 
		5. The goal is to describe the optimal solution in terms of smaller subproblems. These smaller subproblems must be computed before, so we have a dependency order. 
		
	2. **Define the recursive solution (top-down).** Describe the problem that you want to solve recursively, in coherent and precise English—not how to solve that problem, but what problem you’re trying to solve. Without this specification, it is impossible, even in principle, to determine whether your solution is correct.  
	3. **Solution.** Give a clear recursive formula or algorithm for the whole problem in terms of the answers to smaller instances of exactly the same problem.
2. **Build solutions to your recurrence from the bottom up.** Write an algorithm that starts with the base cases of your recurrence and works its way up to the final solution, by considering intermediate subproblems in the correct order. This stage can be broken down into several smaller, relatively mechanical steps:  
	1. **Identify the subproblems.** What are all the different ways your recursive algorithm can call itself, starting with some initial input? For example, the argument to `R…F…` is always an integer between 0 and *n*.  
	2. **Choose a memoization data structure.** Find a data structure that can store the solution to every subproblem you identified in step (a). This is usually but not always a multidimensional array.  
	3. **Identify dependencies.** Except for the base cases, every subproblem depends on other subproblems—which ones? Draw a picture of your data structure, pick a generic element, and draw arrows from each of the other elements it depends on. Then formalize your picture.  
	4. **Find a good evaluation order.** Order the subproblems so that each one comes after the subproblems it depends on. You should consider the base cases first, then the subproblems that depend only on base cases, and so on, eventually building up to the original top-level problem. The dependencies you identified in the previous step define a partial order over the subproblems; you need to find a linear extension of that partial order. Be careful!  
	5. **Analyse space and running time.** The number of distinct subproblems determines the space complexity of your memoized algorithm. To compute the total running time, add up the running times of all possible subproblems, assuming deeper recursive calls are already memoized. You can actually do this immediately after step (a).  
	6. **Write down the algorithm.** You know what order to consider the subproblems, and you know how to solve each subproblem. So do that! If your data structure is an array, this usually means writing a few nested for-loops around your original recurrence, and replacing the recursive calls with array look-ups.

*Of course, you have to prove that each of these steps is correct. If your recurrence is wrong, or if you try to build up answers in the wrong order, your algorithm won’t work!*

### Developing a Dynamic Programming solution

- <b>Step 1: Identify some optimal substructure.</b> The first step is finding some optimal substructure that will inspire our subproblems. Recall: an optimal substructure is the property that an optimal solution is made up of optimal solutions to smaller problems → this is what makes dp possible. 
	- The optimal value/set/path/schedule must contain smaller optimal parts. 
	- Find how we can build an optimal solution from the previous optimal solutions of smaller instances to the problem etc.
- <b>Step 2: Defining the subproblems.</b> Once we’ve observed some optimal substructure, we define the smaller instances or states that represent this relationship. 
	- Assume we know the optimal solutions for the previous parts; how do we get the one for current i?
- <b>Step 3: Deriving a recurrence.</b> The next step is to derive a recurrence relation between the subproblems and some base case(s). That is, identify the relationship between subproblems as a recurrence relation.

Notes:
- Identify the set of subproblems (a smaller instance of the problem that we can compute) that we can use to get the final answer. This is the hardest part! 
- A subproblem does *not* mean the same problem but on a subset of the input. It means a smaller instance, state or case of the same problem — a more specific variant (substructure) of the problem. 
- First, we need some function or data structure that gives/stores the answer to the problem from a given state. For example, let `dp[i]` be the optimal (best) solution to the first i items. Let's say we know `dp[0]`, `dp[1]`, and `dp[2]`. How can we find `dp[3]` given this information? Then we figure out what exactly we would need to know to transition from `dp[2]` to `dp[3]`. And that would be the "substructure" that gives us the optimal solution!

### Setting up a Dynamic Programming solution

A dynamic programming solution must have the following descriptions.

- <b>Subproblem and Recurrence.</b> The subproblem describes exactly what problem you are solving and the recurrence describes how you combine solutions to previously seen subproblems.
- <b>Base case(s) and Final solution(s).</b> The base cases describe special cases that cannot be solved simply with the recurrence, while the final solution(s) is usually a set of subproblems that return the final solution to the original problem.
- <b>Order of computation.</b> The order of computation describes the sequence of sub- problems that you will resolve. Remember that the order of computation needs to be an ordering so that the current subproblem relies only on subproblems that have previously been solved.

For example (LIS):

> [!solution]
> **Subproblems**
> 
> For each $1 ≤ i ≤ n$, let $len(i)$ be the maximum length of an increasing subsequence of $A[1..i]$ which ends with $A[i]$.
> 
> **Recurrence**
> 
> For $i > 1$,
>
> $$\text{len}(i) = 1 + \max_{\substack{j < i \\ A[j] < A[i]}} \text{len}(j)
> $$
>
> **Base case**
> 
> $\text{len}(1) = 1$
> 
> **Order of computation**
> 
> Solve subproblems in increasing order of $i$. 
> 
> **Overall answer**
> 
> The overall LIS is the best of those ending at some element, i.e., $\displaystyle \max_{1 \leq i \leq n} \text{len}(i).$
> 
> **Time complexity**
>
> $O(n)$ subproblems each taking $O(n)$, and overall answer calculated in $O(n)$ time, for a total running time of $O(n^2)$.

### Analysing the correctness and time complexity

To prove the correctness of dynamic programming algorithms, we need to prove that the recurrence is correct. This corresponds to proving that the problem satisfies the first property: optimal substructure. 

To do so, we use the cut and paste argument as follows:

- Consider an optimal solution $S = {s_1,…,s_k}$ to the current subproblem, and suppose that there exist some index $i$ for which $s_i$ is a suboptimal choice made by any optimal solution.
- Replace $s_i$ with $g_i$, which is the choice made by the dynamic programming recurrence, to form a new solution $S^* = {s_1,…,s_{i−1},g_i,s_{i+1},…,s_k}$.
- Deduce that $S^*$ is optimal, which implies that an optimal solution can be derived from optimal subsolutions.

