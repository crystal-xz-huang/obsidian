Related to:
- [IPO - LeetCode](https://leetcode.com/problems/ipo/description/)


![[COMP3121_9101-2.7.pdf]]

## Exchange Argument

Suppose $O$ is optimal and $O \neq G$. Then we can modify $O$ to get a new solution $O'$ that is:
1. No worse than $O$ (still valid and optimal)
2. Closer to $G$ is some measurable way.

**Optimal:** 
Final knowledge level is highest obtainable/achievable value (of all valid solutions) after all projects have been completed in a given sequence of projects.

i.e. There must exist no solution S' such that cost(S') > cost(S) in the case that there are multiple optimal solutions. 

**Valid:** 
We say that a set S of projects is valid if:
- every project in S is distinct, and 
- every project has a requirement of *no greater than* the current knowledge

Let $G=\{g_1,g_2,\dots,g_m\}$ denote the projects selected by our greedy solution, where $g_i$ is the project chosen at step $i$. Similarly, let $O=\{o_1,o_2,\dots,o_{m'}\}$ denote any optimal solution. 

For $t\ge1$, let $G_t=\{g_1,\ldots,g_t\}$ and $O_t=\{o_1,\ldots,o_t\}$ be the partial solutions up to and including step $t$, where $k$ is maximum number of projects we can complete.

Now, suppose that $O\neq G$ (otherwise $O = G$ and we are done). Then there is at least one project in $O$ that is not in $G$, or there is at least one pair of project in $O$ that are in a different order in $G$ (i.e. an inversion). Let $t$ be the first place where they differ, so $g_i=o_i$ for all $i<t$ and $g_t\ne o_t$. 

We will now show that we can transform the optimal solution to match our greedy algorithm's choice at $t$ by either exchanging $o_t$ with $g_t$ or swapping the order of the two projects without decreasing the cost of the optimal solution, thereby proving that the greedy solution is optimal at point $t$. 

First, note that the knowledge level after step $t$ is the sum of the knowledge gainable from a set if projects, regardless of order. Therefore, it suffices to compare the ....

For a subset of projects $S_t=\{s_1,\dots,s_t\}$, let $C(S)$ be the total gain after $t$ steps:

$$
C(S_t) = \sum_{i=1}^{t} \text{gain}({s_i}).
$$

By our greedy algorithm, we choose the project with the highest gain among all available projects that have not yet been chosen and have a requirement no greater than our current knowledge level. Since O has completed the same $t-1$ projects as G, the current knowledge level is the same for both O and G at this point. This implies that $g_t$ is also unlocked (i.e. we have enough knowledge to start) for O and conversely, $o_t$ is unlocked for G. 

We consider two cases.

- Case 1. $g_t$ is not in $O$. In this case, we can exchange $o_t$ for $g_t$ to construct a new solution $O' = \{ o_1, \ldots, o_{t-1}, g_t, o_{t+1}, \ldots, o_{m'}\}$ that is still valid (since $g_t \notin O$) and is also optimal since the gain from $g_t$ is no less than any other available project $o_t$, which follows that our knowledge level from step $t$ onwards in O' is no less than O.
- Case 2. $g_t$ is in $O$. This means that $O$ selects $g_t$ at some later point $j>t$. Note that if $g_t$ was available at time $t$, then it must also be available at a later time $j$ since our knowledge strictly increases with each addition of a project. In other words, if we meet the requirement for a project at $t$, then the project remains unlocked after $t$. In this case, we can simply reorder the two projects and the cost remains the same, which implies O' is no worse than O.

In either case, $O'$ is valid optimal solution. Since $O'$ and $G$ and differ in only a finite number of places, repeating this procedure a finite number of times produces a sequence of optimal solutions where each solution is closer to $G$. The sequence terminates when $G$ is reached, or when 

and $C(O') \geq C(O)$. By a sequence of such swaps, we can always reach the greedy solution $G$ without decreasing its cost. Therefore, no other solution $O$ can achieve a strictly greater overall knowledge level than the greedy solution, and thus $G$ is optimal. 

# Notes

Given a set of $n$ projects numbered from $1$ up to $n$, each with a gain $g_i$ and requirement $r_i$ to meet, along with a maximum project capacity of $k$. The problem is to maximise the sum of gains from completing the projects in the knapsack. Output: A sequence of exactly $k$ projects. 

- A project is **feasible** if $r_i \leq \text{current knowledge}$. 
- A project is **optimal** if (1) it is feasible, and (2) it has the largest gain.

Note that we are guaranteed that there exists at least one feasible solution of length k.
But doesn't mean the solution must be of length k. 

> [!cite] 
> For all parts, the values $s$, $r_i$, and $g_i$ are all positive, and you are guaranteed that there is a sequence of $k$ projects that you can complete.


Input:
- $n$ projects → choose exactly $k$ projects (and $k<n$)
- each project $i$ has an associated:
	- requirement $r_i$ = minimum knowledge to start, 
	- gain $g_i$ = knowledge gained after completion.
- initial knowledge level = $s$

Goal: **Maximize final knowledge level** after completing $k$ projects.

> A project is feasible if $r_i ≤ \text{current knowledge}$

At any moment:
- You can only start projects with $r_i ≤ \text{current knowledge}$
- When you pick one, your knowledge increases by $g_i$.
- Each new project choice expands what you can unlock next ⇒ choice affects what projects become feasible in the future

Because you can repeat, the best long-term strategy is to keep repeating the project that gives you the **largest knowledge gain** among all feasible ones.

# Solution

## Relaxed Design

> [!example|style-bq] Part A
> First, assume that ==we can repeat projects as many times as we would like==. Design an efficient algorithm to find the sequence of projects that maximises your final knowledge level. Provide time complexity analysis and brief discussion of correctness.

#### Remarks

Similar to an **unbounded knapsack problem** with the constraint that we meet the project's requirements: $r_i \leq \text{knowledge}$

#### Brute Force

Algorithm:
1. Sort the $n$ projects in ascending order of $r_i$ so that $r_1 \leq r_2 \leq \cdots \leq r_n$. `O(nlogn)`
2. Repeat the following operations $k$ times:
	1. Among all **feasible** projects ($r_i \leq \text{knowledge}$), always choose the project with the **largest (maximal) gain**. `O(n) linear search`  
	2. Increase our $\text{knowledge}$ by $g_i$ `O(1)`
3. We repeat this process until we have completed $k$ projects. `O(n x k)`

Time Complexity:
- Total time = `O(nk) + O(nlogn) = O(nlogn + nk)` 
- In the worst case, $k = n$ and the time becomes `O(nlogn + n^2) = O(n^2)` (dominated by the cost of linear search)

#### Greedy Approach

Among all feasible projects, the greedy choice is the project with the largest gain. This means we should repeatedly choose the optimal project until we have completed $k$ projects. To implement the algorithm efficiently, we need to be able to ==find all currently feasible projects quickly and determine the most optimal job quickly ⇒ under linear time==. 

Using a max-heap to maintain all currently feasible projects allows us to select the project with the largest knowledge gain in $O(\log n)$ time.

To efficiently determine which projects are available and which one offers the largest gain, we maintain a **max-heap** of all currently feasible projects. Each time our knowledge increases (i.e. at each step), we (1) **push** the newly feasible projects onto the heap and (2) **peek (query)** the top of the heap to find the project with the maximum gain. Since the projects are sorted by requirements, we can initialise a pointer to the first unavailable project, and increment it as we unlock more projects. 

> Note that we `peek` under the assumption "we can repeat projects as many times as we would like", because we do NOT want to remove projects from further consideration (i.e. can select the same project multiple times). 
> 
> For the strict design assumption "projects cannot be repeated", we `pop` instead of `peek`, to REMOVE every project that is selected. 

Time Complexity:
- Sorting all the projects by their requirements will take `O(nlogn)`
- Each project is inserted into the heap at most once and each insertion takes O(logn) time to execute; total cost is `O(nlogn)`
- We make a total of k queries for the maximum project and each query takes O(1) time to execute; total cost is `O(k)` 
- Total time = `O(nlogn + k)` = `O(nlogn)` since n ≤ k (dominated by the overhead cost for heap insertion)

## Strict Design

> [!example|style-bq] Part B
> Now, assume projects cannot be repeated. Design an algorithm that runs in $O(n\log n)$ time to find the sequence of projects that maximise your final knowledge level. 

Use the same approach as before. However, as projects cannot be repeated, we can `pop` (remove) the project with the maximum gain from the heap instead of peeking.

Time Complexity:
- Sorting all the projects by their requirements will take `O(nlogn)`
- Each project is inserted into the heap at most once and each insertion takes O(logn) time to execute; total cost is `O(nlogn)`
- We make a total of k deletions, and each deletion takes O(logn) time to execute; total cost is `O(klogn)` 
- Total time = `O((n+k)logn)` = `O(nlogn)` since k ≤ n 

    Each project is inserted into the heap at most once, for a total of $n$ projects, and each step accesses the maximum project at the top of the heap, for a total of $k$ steps; therefore, we perform at most $O(n)$ heap operations where each operation takes $O(\log n)$ time to execute. This gives the algorithm an overall running time of $O(n\log n)$.

## Proof of Correctness for Strict Design

- Let `C` be the corresponding cost function which we seek to maximise
- Show that the cost of greedy at its optimal state is at least any optimal solutions cost at its starting state. That is, `C(G1) ≥ C(O1)`.
- Then suppose `C(Gk) ≥ C(Ok)` and prove that `C(Gk+1) ≥ C(Ok+1)`
	- Construct a sequence of inequalities that proves this statement.
- After induction, we have shown that `C(G) ≥ C(O)`. Since `O` was defined as an optimal solution, we also have that `C(G) ≤ C(O)`. Therefore, we have that `C(G) = C(O)` which implies that `G` is optimal.


Let $G = \{ g_1, g_2, \dots, g_m \}$ and $O = \{ o_1, o_2, \dots, o_{m'} \}$ denote the sequence of chosen by the greedy and optimal solutions respectively. 

Let f(G) = knowledge gain in G orderings of projects.

Show that f(G) ≥ f(O) for all 0 < i < k 

Base case (i = 1)
Since the greedy solution selects the project with the highest gain among all feasible projects, it certainly must be the case that f(G_i) ≤ f(O_i).

Inductive Hypothesis
Suppose now that f(G_i) ≥ f(O_i) is true for all 1 ≤ i ≤ k - 1.
Assume that for some k-1 ≥ 1, we have f(G_k-1) ≥ f(O_k-1).
We will now prove that f(G_k) ≥ f(O_k).

If f(G_k-1) ≥ f(O_k-1) then any project feasible for O at k-1 is also feasible for G. 
Since our greedy solution selects the project with the maximum gain that is also feasible, then
it certainly must be the case that f(G_k) ≥ f(O_k).
So we have for all i ≤ k, that f(G_i) ≥ f(O_i). 

---

# Further Reading & References

From LeetCode:

#### Intuition

What project is the best to choose when we can finish only one? We want the one that yields the maximum profit.  However, the choice is constrained – the project is available only if $r_i ≤ \text{current knowledge}$. Thus the optimal project is the most profitable among those for which we have enough knowledge to start. If no project is available, we don't have the option to begin any, and the answer is `0`.

It can be generalized for arbitrary k. First, we greedily choose the most profitable available project. Then our knowledge increases by the profit of this project, and some new projects that were unavailable before might become available now. If we choose a project other than the most profitable one, our capital increases by a value less than the maximum possible, and fewer new options become available. It means we should greedily choose the maximum profit every time. We can repeat this process of choosing the most profitable project and then updating the projects we can afford until we finish k projects or cannot afford any new ones.

Now the problem breaks into two parts: finding new available projects after finishing the previous one and finding the most profitable available project.

To handle the first part, we make the following observation: when our knowledge grows, we have more options to choose from, and the smaller `knowledge` a project requires, the sooner it becomes available. Thus we can sort the projects by increasing `knowledge` and keep a pointer to the first unavailable project. As we gain more knowledge, we can increment this pointer to unlock more projects.

For the second part of the problem, we need a data structure for maintaining available projects that can perform the following operations:

- insert an element (for adding a new available project),
- get the maximum element (for choosing the best project),
- delete the maximum element (for finishing the best project).

The data structure with the above properties is a priority queue. The priority of a project is its profit. 

#### Algorithm

1. Sort the projects by increasing `r`. Keep a pointer `ptr` to the first unavailable project in the sorted array.
2. Maintain a priority queue for the profits of available projects. Initially, the priority queue is empty.
3. Do the following k times:
    - Add to the priority queue the profits of the newly available projects. We move the pointer through the sorted array when new projects become available.
    - If the priority queue is empty, terminate the algorithm.
    - The maximum value in the priority queue is the profit of the project we will start now. Increase our knowledge by this value. Delete it so since we can not use it anymore.

```python
class Solution:
    def findMaximizedCapital(self, k: int, w: int, profits: List[int],
                             capital: List[int]) -> int:
        n = len(profits)
        projects = list(zip(capital, profits))
        projects.sort()
        # heapq is a min heap, but we need a max heap
        # so we will store negated elements
        q = []
        ptr = 0
        for i in range(k):
            while ptr < n and projects[ptr][0] <= w:
                # push a negated element
                heappush(q, -projects[ptr][1])
                ptr += 1
            if not q:
                break
            # pop a negated element
            w += -heappop(q)
        return w
```

Let $n$ be the number of projects.

- Time complexity: O(nlogn). Sorting the projects by increasing `knowledge` takes O(nlogn) time. Also, we perform O(n) operations with the priority queue, each in O(logn).
- Space complexity: O(n). The sorted array of projects and the priority queue take linear space.


