> [!abstract]
> A quiz presenting proof methods for greedy algorithms, with an example in a cloze passage: [https://moodle.telt.unsw.edu.au/mod/quiz/view.php?id=8074872](https://moodle.telt.unsw.edu.au/mod/quiz/view.php?id=8074872). This quiz closes at 11pm on the 11th of October, 2025.

As you have learned in lectures, greedy algorithms are algorithms that are underpinned by a (often simple) heuristic. This heuristic is invoked at each iteration, and takes some locally optimal choice, with respect to a metric and the current input.

The point of this quiz is to assist in proving greedy algorithms, because while the termination of the algorithm induced by the heuristic may be trivial, its correctness is generally not. Recall that from your work earlier in the course, that **every** algorithm should be accompanied by a proof of correctness; your solutions should be considered incomplete otherwise. This is particularly important, because while there are some problems that admit greedy solutions, this is not true in the general case. Often, greedy algorithms fail, and given an incorrect initial value, when invoked, reach a locally optimal solution, and incorrectly declare it globally optimal.

There are two proof techniques covered in lectures; _greedy stays ahead_ and the _exchange argument_.

---
##### Greedy Stays Ahead

At each iteration where the greedy algorithm chooses its next option, we can define a metric that measures the cost of a particular solution at that point.

Our goal is to show that at each iteration, that greedy is at least as good as any optimal solution. (Note that we can compare with other solutions as well, but there's redundancy here; when proving the optimality of a greedy algorithm, we naturally only care about other optimal solutions.)

We offer a scaffold here, since this form of proof is not so widely applied. Most of its applications in this course are on problems that are linear in nature. Often, these problems have equivalent exchange arguments, that may be simpler. (In fact, both approaches are provably identical - but we won't cover that in this quiz.)

Here, we focus on a minimisation problem, since any maximisation problem has an equivalent minimisation problem by negating the cost function (or alternatively, changing the direction of the inequality).

> [!example]
>  Let $G=\{g_1,g_2,\dots,g_m\}$ and $O=\{o_1,o_2,\dots,o_{m'}\}$ be the greedy and optimal solutions respectively, where $g$ Let $G_k=\{g_1,g_2,\dots,g_k\}$ and $O_k=\{o_1,o_2,\dots,o_k\}$ be the truncated solutions, up to and including step $k < m$.
>  
> Note that for $G_{k+1}$, the step $k+1$ is generated by applying the greedy heuristic to $G_k$, that is: $G_{k+1} = G_k \cup { g_{k+1} }$.
> 
> Let $C(\cdot)$ be the cost function we seek to minimise.
> 
> We need to show that the cost of the greedy solution at its initial state is **at most** the cost of any optimal solution at its initial state. That is, $C(G_1) \le C(O_1)$.
> 
> Then, suppose that $C(G_k) \le C(O_k)$. We are required to prove that $C(G_{k+1}) \le C(O_{k+1})$.
> 
> Typically, we use properties of the heuristic, $G_k$, $O_k$, and other details supplied in the question to construct a sequence of inequalities that proves this statement. It is often helpful to express $C(G_{k+1})$ as some function of $C(G_k)$ (and similarly for $O$) when proving this. Recall that by definition,  $C(G_{k+1}) = C(G_k \cup { g_{k+1} })$, so breaking the cost function into its constituent parts, if they exist, is helpful to constructing such a sequence.
> 
> Reaching the conclusion above means that, inductively, we have shown that $C(G) \le C(O)$. Since $O$ was defined as an optimal solution, we also have that $C(G) \le C(O)$.
> Therefore, we have that $C(G) = C(O)$, which implies that $G$ is **optimal**.

---
##### Exchange Argument

We reuse the idea of a cost function; but we instead apply this idea to only entire sequences. That is, we do not analyse the cost of truncated solutions.

The point of the exchange argument is to show that the greedy solution is optimal through a sequence of transformations that does not impact optimality or validity. That is, for each valid and optimal solution $O$ to the original problem, there exists some sequence of valid and optimal solutions ${O', …, O^{(n)}}$ such that:

- for all $1 \le i$, we have that $O^{(i)}$ and $O^{(i+1)}$ differ by exactly one element;

- for all $1 \le i \le n$, we have that $C(O) = C(G) = C(O^{(i)})$, i.e. each intermediate solution is optimal;

- for all $1 \le i \le n$, we have that $O^{(i)}$ is a valid solution.

This shows that $G$ can be constructed from $O$ through this sequence of solutions, i.e.  
$O \to O' \to \cdots \to O^{(n)} \to G$, and thus $G$ must also be optimal.

We offer a scaffold here, followed by an example.


> [!example]
> Let $G=\{g_1,g_2,\dots,g_m\}$  and $O=\{o_1,o_2,\dots,o_{m'}\}$ be the greedy and optimal solutions respectively. Let $G_k=\{g_1,\dots,g_k\}$ and $O_k=\{o_1,\dots,o_k\}$ be the truncated solutions, up to and including step $k < m$.
> 
> Suppose that $k$ is the first place where $G$ and $O$ disagree.  
> That is, $G_{k-1} = O_{k-1}$.
> 
> Argue that $O' = { o_1, …, o_{k-1}, g_k, o_{k+1}, …, o_{m'} }$ is both valid and optimal.  Validity should mean that in the context of the problem, this solution is possible.  In particular, since the solution is mostly the same as an optimal solution, we should verify that the transitions $o_{k-1} \to g_k$ and $g_k \to o_{k+1}$ are possible, and that the solution as a whole satisfies the constraints of the problem. This shows that $O'$ is also valid.
> 
> Optimality means verifying that the cost of this new solution is also optimal.
> 
> If we do not prove both of the above, we cannot argue that $G$ is optimal in this way.
> 
> Repeat the argument until we reach $G$.  This implies that $C(O) = C(O') = \cdots = C(O^{(n)}) = C(G)$ as outlined above, and thus greedy is optimal.
> 

---
##### When might you want to use greedy stays ahead (GSA) over exchange arguments, and vice versa? 

GSAs are typically used less. The canonical situations where it appears are in one-dimensional problems \[ie. can be explained by a point moving on a line or some equivalent\], or problems where a suitable cost function is the number of iterations. The difficulty in usage typically lies in breaking down your cost function (especially if adding the next iteration inside your cost function for the inductive step) yields some non-elementary function. It also relies on you defining a suitable cost function to begin with. Moreover, the latter instance (counting iterations) often has an objectively easier exchange proof (see if you can prove Q3 by GSA; finding a metric is non-trivial). 

Exchange arguments are typically better in most instances. At each iteration, a greedy solution either deviates from some optimal solution, or it doesn't. At each deviation, we can exchange this with some sequence so this deviation no longer exists to generate our exchange. This generally fits into our paradigm of greedy making one choice out of (im)possibly many at each iteration. 

Informally, if your problem admits a greedy solution in the first place, it's unlikely to have a cleaner GSA proof, unless it falls into one of the aforementioned forms. 

##### How can I (informally) verify my heuristic? 

You might want to consider if your heuristic is becoming obnoxiously difficult to prove. This may mean your approach is not correct, since not generating the conclusion means that your solution is incorrect. But of course, proving a negative is often an impossibility. You might also want to consider what gaps may exist in your approach; can you generate counterexamples/edge cases? If you can generate a counterexample, you have proven that your heuristic is wrong.