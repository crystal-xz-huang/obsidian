---
aliases:
  - 3.3 [PS] Thinking About DP
modules:
  - "[[mocs/Modules/Dynamic Programming|Module 3: Dynamic Programming]]"
---
> [!summary]
> An introductory self-paced quiz for DP:Â 
> 
> - [https://moodle.telt.unsw.edu.au/mod/quiz/view.php?id=8074885](https://moodle.telt.unsw.edu.au/mod/quiz/view.php?id=8074885)
> 
> This quiz closes at 11pm on the 18th of October, 2025.

# Thinking About DP

###### DP solutions as Graph Traversals on a DAG

In dynamic programming, we aim to solve problems efficiently by first solving subproblems. These subproblems are solved in some specified order, where future subproblems are some function of past subproblems. We store solutions to previously computed subproblems so that we can retrieve inputs to future subproblems efficiently.Â 

However, in order to solve the current subproblem, it must not depend on any subproblem that is not yet solved. Therefore, it makes a lot of sense to consider dynamic programming solutions as graph traversals on a directed acyclic graph (DAG). We show that this is generally true, by observing that a) cycles are bad, and that b) the graph is directed.

###### Why are cycles bad?

Denote our current subproblem $P(i)$ , and suppose we need information from the solution of some other subproblem $P(j)$, which in turn needs information from the solution of some other subproblem $P(k)$, and so on. Suppose the subproblem structure has a cycle. Then, there will be a subproblem in this sequence of dependencies, sayÂ $P(\ell)$, that depends onÂ $P(i)$. This implies we have some chain of dependencies of the form

$$\ldots \rightarrow P(i) \rightarrow P(j) \rightarrow P(k) \rightarrow \ldots \rightarrow P(\ell) \rightarrow P(i).$$

From the above, we can conclude that solvingÂ $P(i)$Â requiresÂ $P(i)$Â to be solved, which is impossible. Therefore, our supposition cannot hold and so the subproblem structure must be acyclic.

Notably, this means that there must be some set of subproblemsÂ $S$, such that every subproblem inÂ $S$Â is not dependent on any other subproblem; these are precisely our base cases!

###### Why is the graph directed?

Denote our current subproblemÂ $P(i)$, and suppose that the underlying subproblem structure is undirected. Then, consider the problems related toÂ $P(i)$; we only care to traverse (and thus solve) the problems whichÂ $P(i)$Â is dependent on prior to traversingÂ $P(i)$. This gives a natural ordering, and this ordering imposes a natural direction on subproblems.

Then, when we draw our DAG, our nodes are subproblems and our edges are immediate subproblem dependencies. (For those familiar, it may make sense to visualise this as having a strict partial order with subproblems forming the ground set, and the binary relation being subproblem dependency, with the graph being the Hasse diagram.) As an exercise, you may want consider why we do not care about non-immediate subproblem dependencies and hence why they are not drawn in our DAG.

###### Motivation

Lastly, we provide intuition on how we may analogise other components of dynamic programming solutions to our DAG.Â 

Recall that from the previous two notes, that our nodes are subproblems; that nodes with no incoming edges are our base cases, and that edges represent our subproblem dependencies (implicitly giving an order of computation,Â _topological order_, which was defined in Module 3).

- <b>Recurrence</b>: Suppose our DAG is denoted byÂ $G(V,E)$, and the current subproblem we want to solve is represented by the nodeÂ $v$. LetÂ $U = \{ u : (u,v) \in E \}$Â i.e., the set of all subproblemsÂ $v$Â depends on. Then, our recurrence is some function on the elements ofÂ $U$, and the information supplied atÂ $v$Â prior to its solving.  
      
    Explicitly computing our recurrence allows us to solveÂ ğ‘£.

- <b>Time complexity</b>: The time complexity of our algorithm is the time taken to traverse the DAG, aggregated with the aggregate time to solve subproblems at all the nodes.

###### Longest Increasing Subsequence (LIS) problem

Now, let's use the motivation to solve a dynamic programming problem that you have seen before in a different way.

Recall the [[Longest Increasing Subsequence]] problem from lectures, where, given an array of values $A[1..n]$, our task was to determine a subsequence (not necessarily contiguous) of maximum length, in which the values in the subsequence are strictly increasing. This was solved by defining the following:

- **Subproblem definition**:Â $\textsf{LIS}(ğ‘–)$Â denotes the maximum length of an increasing subsequence ofÂ $A[1..i]$Â which ends withÂ $A[i]$.  
- **Base case**:Â $\textsf{LIS}(1)=1$, since we trivially include the first value itself to form the subsequence.
- **Recurrence**:Â $\displaystyle \textsf{LIS}(i)=1+ \max_{\substack{j < i \\ A[j] < A[i]}} \textsf{LIS}(j)$.
- **Order of Computation**: increasing order ofÂ $i$.
- **Final solution**:Â $\displaystyle \max_{\substack{1 \leq i \leq n}} \textsf{LIS}(i)$.
- **Time complexity**:Â $O(n^2)$.

## LIS as Longest Path on a DAG

Construct a graph $G = (V, E)$, where the vertex set $V$ has $n$ vertices $v_i$, one for each $i$, and our edge set $E$ has a directed edge $u \to v$ for $u, v \in V$ if the index at $u$ is less than that at $v$, and the value in $A$ indexed at $u$â€™s index is less than that at $v$.

Then, at each vertex $v_i$, we are required to compute $\text{LIS}(i)$. We can solve in topological order, or in increasing order of $i$. *Note: you may want to argue why increasing order of $i$ is also a topological order here*.

Suppose we solve in topological order. Then, our base cases are vertices with no incoming edges. Then, we have that the solution at these vertices at 1. 

> If we solve in topological order, the base cases are the vertices with no incoming edges. The solution at these vertices is $1$.

We then invoke the recurrence and solve at each vertex.

We now show that this reduces to solving the longest path on a DAG. To do this, we show that every increasing sequence of $A$ can be given by a path in $G$, and vice versa.

- $(\Longrightarrow)$ Suppose we have an increasing subsequence $\{a_1, a_2, \dots, a_m\}$. Then, since the indices are increasing, we have an edge between vertices representing consecutive indices in the subsequence if and only if the value in $A$ indexed at $(iâˆ’1)$â€™s index is less than that at $i$. By construction, this holds for each consecutive pair. Then, since we have a vertex for each element, and we have an edge between all pairs of vertices that represent consecutive elements of our sequence, this directly corresponds to a path in $G$. We can thus conclude that the length of the corresponding path is equal to the length of the subsequence, where length is measured by number of vertices visited. 
- $(\Longleftarrow)$ Suppose we have a path in $G$. Then, by construction, the vertex sequence represents some item sequence in $A$, where each successive element has index more than the previous (that is, we have a subsequence of $A$), and each successive element has value more than the previous (that is, the subsequence is increasing). Similarly, we conclude that the corresponding subsequence length is equal to the number of vertices in the path.
- Thus, we conclude that any maximum-length increasing subsequence in $A$ is also a maximum-length path in $G$.

Hence this is the same as solving the longest path problem on a DAG, which is computable in time linear in the size of the graph. 

The time complexity is thus $O(|V| + |E|)$. Since $G$ is a DAG, $|E|$ is at most quadratic in $|V|$, so the overall runtime is $O(n^2)$, recalling that $|V| = n$.

## Proof of Correctness

We now show the correctness of our algorithm.

Our base cases for the longest path algorithm were trivially correct, since the longest path finishing at a vertex with no incoming vertices must be of length $1$.

Then, suppose that the longest path finishing at all vertices up to $v_k$ have been solved correctly, i.e. that we have all optimal solutions for $\text{LongestPath}(v_i)$, $1 \le i \le k$.

Then, at vertex $v_{k+1}$, note that any path ending at $v_{k+1}$ must arrive through a predecessor of $v_{k+1}$. Suppose that we have an optimal solution for the longest path ending at $v_{k+1}$, and suppose this is constructed using a suboptimal length path ending at a predecessor of $v_{k+1}$.

Then, we can truncate $v_{k+1}$, replace the suboptimal prefix of the path with the longest path ending at any of $v_{k+1}$â€™s predecessors, to produce a longer prefix. These have all been solved, so selecting from this set gives the global best solution for a predecessor of $v_{k+1}$. Appending $v_{k+1}$ to this optimal prefix then produces a necessarily longer path than the path we started with, contradicting the optimality of our original solution. Thus, we cannot construct a longest path ending at $v_{k+1}$ with a suboptimal length prefix, and hence our solution at $v_{k+1}$ must necessarily be

$$
1 + \max_{(u,v_{k+1}) \in E} \text{LongestPath}(u).
$$

Thus, by showing the correctness of our longest-path-on-a-DAG algorithm, and by showing that we have a valid reduction from LIS to longest path on a DAG, we have solved LIS correctly.

*Note that we have implicitly used the cut-and-paste argument in our proof, which you may want to replicate in your own proofs.*

